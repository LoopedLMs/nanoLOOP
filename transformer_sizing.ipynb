{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Looped Transformer Theoretical Model\n\nThis notebook estimates the number of FLOPs, parameters, peak memory footprint, checkpoint size, etc. for our looped (depth-recurrent) transformer architecture: **prelude → recur (×n_loop) → coda**.\n\nKey insight: recur block weights are *shared* across loop iterations, so the model has fewer unique parameters than its effective depth suggests."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model config (matches GPTConfig defaults in model.py)\nblock_size = 1024\nvocab_size = 50304  # GPT-2 vocab_size of 50257, padded to nearest multiple of 64\nn_prelude = 2       # unique layers run once before looping\nn_block = 2         # unique layers in the recurrent block (shared across loops)\nn_coda = 2          # unique layers run once after looping\nn_loop = 4          # number of recurrence iterations\nn_head = 12\nn_embd = 768\nbias = False\ninput_injection = \"inject\"  # \"inject\", \"inject_random\", or \"passthrough\"\nbptt_k = None               # truncate backprop to last k recurrences (None = full)\nassert not bias, \"this notebook assumes bias=False just for simplicity\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def params():\n    \"\"\"estimates the number of unique parameters in the model\"\"\"\n    out = OrderedDict()\n\n    # token and position embeddings\n    out['embedding/position'] = n_embd * block_size\n    out['embedding/token'] = n_embd * vocab_size\n    out['embedding'] = out['embedding/position'] + out['embedding/token']\n\n    # per-block parameters (same structure for prelude, recur, coda)\n    out['attention/ln'] = n_embd  # RMSNorm weight (no bias)\n    out['attention/kqv'] = n_embd * 3 * n_embd\n    out['attention/proj'] = n_embd ** 2\n    out['attention'] = out['attention/ln'] + out['attention/kqv'] + out['attention/proj']\n\n    ffw_size = 4 * n_embd\n    out['mlp/ln'] = n_embd  # RMSNorm weight (no bias)\n    out['mlp/ffw'] = n_embd * ffw_size\n    out['mlp/proj'] = ffw_size * n_embd\n    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n\n    out['block'] = out['attention'] + out['mlp']\n\n    # architecture sections — recur weights are shared across loop iterations\n    out['prelude'] = n_prelude * out['block']\n    out['recur'] = n_block * out['block']\n    out['coda'] = n_coda * out['block']\n    out['all_blocks'] = out['prelude'] + out['recur'] + out['coda']\n\n    # looped transformer extras\n    out['norm_recur'] = n_embd  # RMSNorm after each recurrence\n    if input_injection in (\"inject\", \"inject_random\"):\n        out['inject'] = 2 * n_embd * n_embd  # Linear(2*n_embd, n_embd, bias=False)\n    else:\n        out['inject'] = 0\n    out['ln_f'] = n_embd  # final RMSNorm\n    out['lm_head'] = 0  # weight-tied with embedding/token\n\n    # total\n    out['total'] = out['embedding'] + out['all_blocks'] + out['norm_recur'] + out['inject'] + out['ln_f'] + out['lm_head']\n\n    return out\n\np = params()\nparams_total = p['total']\nn_unique_layers = n_prelude + n_block + n_coda\nn_effective_layers = n_prelude + n_block * n_loop + n_coda\nprint(f\"unique layers: {n_unique_layers}, effective depth: {n_effective_layers} (looping {n_block} blocks × {n_loop})\")\nprint(f\"total parameters: {params_total:,}\\n\")\nprint(f\"{'name':24s} {'params':>12s} {'ratio (%)':>10s}\")\nfor k, v in p.items():\n    print(f\"{k:24s} {v:12,d} {v/params_total*100:10.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# checkpoint size estimate\n# params stored in fp32; AdamW keeps 2 additional fp32 buffers per param (momentum + variance)\nparams_bytes = params_total * 4\nparams_and_buffers_bytes = params_bytes + 2 * params_bytes\nprint(f\"est checkpoint size (full optimizer state, fp32): {params_and_buffers_bytes/1e9:.2f} GB\")\nprint(f\"est checkpoint size (weights only, bf16):         {params_total*2/1e9:.2f} GB\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "gpu_memory = 80e9  # A100-SXM4-80GB\nprint(f\"memory ratio for parameters + optimizer state: {params_and_buffers_bytes / gpu_memory * 100:.2f}%\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. not that much of the memory for this tiny model, most of the memory is activations (forward and backward). This of course changes dramatically for larger and larger models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's estimate FLOPs for a single forward+backward pass. In the looped architecture, the recur blocks and inject layer execute `n_loop` times in the forward pass, but backpropagation may only flow through the last `bptt_k` iterations (truncated BPTT)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def flops():\n    \"\"\"estimate FLOPs for one forward+backward pass of a single sequence.\n\n    Accounts for looped architecture: recur blocks + inject run n_loop times\n    forward, but only bptt_k times backward (truncated BPTT).\"\"\"\n    out = OrderedDict()\n    head_size = n_embd // n_head\n    ffw_size = 4 * n_embd\n    _bptt_k = min(bptt_k, n_loop) if bptt_k is not None else n_loop\n\n    # per-block FLOPs (for one execution of one block on one sequence)\n    block_attn = (\n        2 * block_size * (n_embd * 3 * n_embd)                     # kqv projection\n        + 2 * block_size * block_size * n_embd                      # Q @ K^T scores\n        + 2 * n_head * (block_size * block_size * head_size)        # attn @ V reduce\n        + 2 * block_size * (n_embd * n_embd)                        # output projection\n    )\n    block_mlp = (\n        2 * block_size * (n_embd * ffw_size)                        # ffw up\n        + 2 * block_size * (ffw_size * n_embd)                      # ffw down\n    )\n    block_flops = block_attn + block_mlp\n    inject_flops = 2 * block_size * (2 * n_embd * n_embd)           # inject linear\n\n    # forward pass\n    out['fwd/prelude'] = n_prelude * block_flops\n    out['fwd/recur'] = n_block * n_loop * block_flops\n    out['fwd/coda'] = n_coda * block_flops\n    out['fwd/inject'] = n_loop * inject_flops if input_injection in (\"inject\", \"inject_random\") else 0\n    out['fwd/dense'] = 2 * block_size * (n_embd * vocab_size)\n    out['forward_total'] = sum(v for k, v in out.items() if k.startswith('fwd/'))\n\n    # backward pass (2× forward FLOPs, but recur/inject only through bptt_k iterations)\n    out['bwd/prelude'] = 2 * n_prelude * block_flops\n    out['bwd/recur'] = 2 * n_block * _bptt_k * block_flops\n    out['bwd/coda'] = 2 * n_coda * block_flops\n    out['bwd/inject'] = 2 * _bptt_k * inject_flops if input_injection in (\"inject\", \"inject_random\") else 0\n    out['bwd/dense'] = 2 * out['fwd/dense']\n    out['backward_total'] = sum(v for k, v in out.items() if k.startswith('bwd/'))\n\n    out['total'] = out['forward_total'] + out['backward_total']\n    return out\n\nf = flops()\nprint(f\"{'name':20s} {'flops':>18s} {'ratio (% of fwd)':>18s}\")\nfor k, v in f.items():\n    print(f\"{k:20s} {v:18,d} {v/f['forward_total']*100:18.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def model_flops_per_fwdbwd():\n    \"\"\"estimate total FLOPs per forward+backward pass using the PaLM-style formula,\n    adapted for looped architecture. This matches model.estimate_mfu() logic.\"\"\"\n    p = params()\n    _bptt_k = min(bptt_k, n_loop) if bptt_k is not None else n_loop\n    H, Q, T = n_head, n_embd // n_head, block_size\n\n    # split params into \"run once\" vs \"reused per loop iteration\"\n    # N = non-position-embedding params (matches model.get_num_params())\n    N = p['total'] - p['embedding/position']\n    once_params = N - p['recur'] - p['inject']\n    reused_params = p['recur'] + p['inject']\n\n    # matmul FLOPs: 2× per param per token fwd, 4× per param per token bwd\n    fwd_matmul = 2 * (once_params + reused_params * n_loop)\n    bwd_matmul = 4 * (once_params + reused_params * _bptt_k)\n    matmul_flops = fwd_matmul + bwd_matmul\n\n    # attention FLOPs (Q@K^T and attn@V, not captured in param count)\n    attn_fwd = 2 * (n_prelude + n_block * n_loop + n_coda) * (2 * H * Q * T)\n    attn_bwd = 4 * (n_prelude + n_block * _bptt_k + n_coda) * (2 * H * Q * T)\n    attn_flops = attn_fwd + attn_bwd\n\n    return (matmul_flops + attn_flops) * T\n\nmf = model_flops_per_fwdbwd()\ndetailed = flops()['total']\nprint(f\"PaLM-style estimate: {mf:,d}\")\nprint(f\"detailed estimate:   {detailed:,d}\")\nprint(f\"ratio: {mf/detailed:.4f}\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "The two estimates are close, giving confidence in our FLOPs math. The small discrepancy comes from the PaLM formula treating all parameters uniformly (including layer norms), while the detailed count only tracks weight matrix FLOPs.\n\nNow let's estimate model FLOPs utilization (MFU). A100-SXM4-80GB is cited at 312 TFLOPS bfloat16 on tensor cores."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# plug in your measured values here\nbatch_size = 100  # total batch size (micro_batch × grad_accum)\nmeasured_time = 1.0  # seconds per iteration — update with your measurement!\n\nmeasured_throughput = batch_size / measured_time\nflops_achieved = f['total'] * measured_throughput\n\na100_flops_promised = 312e12  # A100 bfloat16 peak TFLOPS\nprint(f\"MFU: {flops_achieved / a100_flops_promised * 100:.2f}%\")\nprint(\"(update batch_size and measured_time with your actual numbers)\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we'd prefer to be somewhere around 50%+, and not just for a single GPU but for an entire DDP run. So we still have some work to do, but at least we're within a factor of ~2X of what is achievable with this GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# training cost estimate\ntokens_num = 300e9  # dataset size in tokens\nnum_gpus = 2  # 2×A100-SXM4-80GB\na100_flops = 312e12\nassumed_mfu = 0.3\nflops_throughput = a100_flops * num_gpus * assumed_mfu\n\n# for looped models, naive 6ND underestimates because each token goes through\n# the recur block n_loop times. Use our detailed FLOPs calculation instead.\nflops_per_token = flops()['total'] / block_size\ntotal_flops = flops_per_token * tokens_num\ntime_needed_s = total_flops / flops_throughput\nprint(f\"training time estimate: {time_needed_s/3600/24:.2f} days\")\nprint(f\"  ({tokens_num/1e9:.0f}B tokens, {num_gpus}×A100-80GB, {assumed_mfu*100:.0f}% MFU)\")\n\n# compare with naive 6ND (ignores weight reuse from looping)\nN = params()['total'] - params()['embedding/position']\nnaive_6nd_flops = 6 * N * tokens_num\nnaive_time = naive_6nd_flops / flops_throughput\nprint(f\"\\nnaive 6ND estimate: {naive_time/3600/24:.2f} days\")\nprint(f\"  (underestimates looped models by {total_flops/naive_6nd_flops:.1f}×)\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "The 6ND formula assumes each token does 6 FLOPs per unique parameter, but in a looped model each token passes through the recur block `n_loop` times, so the actual compute per token is higher than the unique parameter count suggests. This is the whole point — we get more \"effective depth\" (compute) from fewer parameters."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, FLOPs are just one constraint, the other that we have to keep a close track of is the memory bandwidth. TODO estimate LOAD/STORE costs of our model later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}